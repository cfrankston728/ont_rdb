{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ont_rdb_explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "Also set the path to ont_rdb and the name of your snakemake profile for processing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import informant_class\n",
    "#import swifter\n",
    "from informant_class import *\n",
    "# Adjust this to the directory storing the ont_rdb package.\n",
    "ont_rdb_path = \"/home/groups/CEDAR/franksto/ont_rdb/ont_rdb\"\n",
    "\n",
    "# Set the snakemake profile you wish to use:\n",
    "snakemake_profile = 'mamba'\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import importlib\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting swifter\n",
      "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /home/groups/CEDAR/franksto/local_envs/ont_rdb_env/lib/python3.12/site-packages (from swifter) (2.2.2)\n",
      "Requirement already satisfied: psutil>=5.6.6 in /home/groups/CEDAR/franksto/local_envs/ont_rdb_env/lib/python3.12/site-packages (from swifter) (6.0.0)\n",
      "Collecting dask>=2.10.0 (from dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading dask-2024.7.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.33.0 (from swifter)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=8.1 in /home/groups/CEDAR/franksto/local_envs/ont_rdb_env/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.1.7)\n",
      "Collecting cloudpickle>=1.5.0 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting fsspec>=2021.09.0 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/groups/CEDAR/franksto/local_envs/ont_rdb_env/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (24.1)\n",
      "Collecting partd>=1.4.0 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/groups/CEDAR/franksto/local_envs/ont_rdb_env/lib/python3.12/site-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.1)\n",
      "Collecting toolz>=0.10.0 (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting dask-expr<1.2,>=1.1 (from dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading dask_expr-1.1.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/groups/CEDAR/franksto/local_envs/ont_rdb_env/lib/python3.12/site-packages (from pandas>=1.0.0->swifter) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/groups/CEDAR/franksto/local_envs/ont_rdb_env/lib/python3.12/site-packages (from pandas>=1.0.0->swifter) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/groups/CEDAR/franksto/local_envs/ont_rdb_env/lib/python3.12/site-packages (from pandas>=1.0.0->swifter) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/groups/CEDAR/franksto/local_envs/ont_rdb_env/lib/python3.12/site-packages (from pandas>=1.0.0->swifter) (2024.1)\n",
      "Collecting pyarrow>=7.0.0 (from dask-expr<1.2,>=1.1->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading pyarrow-17.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting locket (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/groups/CEDAR/franksto/local_envs/ont_rdb_env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.16.0)\n",
      "Downloading dask-2024.7.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Downloading dask_expr-1.1.9-py3-none-any.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Building wheels for collected packages: swifter\n",
      "  Building wheel for swifter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16506 sha256=9e90a040f61a1f8a83776e2dac8bad1e3913c4c8d331b80df016a9e32f41b835\n",
      "  Stored in directory: /home/users/franksto/.cache/pip/wheels/d9/31/ff/ff51141a088571a9f672449e5aad5ea8bb35ca5d95ba135f30\n",
      "Successfully built swifter\n",
      "Installing collected packages: tqdm, toolz, pyarrow, locket, fsspec, cloudpickle, partd, dask, dask-expr, swifter\n",
      "Successfully installed cloudpickle-3.0.0 dask-2024.7.1 dask-expr-1.1.9 fsspec-2024.6.1 locket-1.0.0 partd-1.4.2 pyarrow-17.0.0 swifter-1.4.0 toolz-0.12.1 tqdm-4.66.4\n"
     ]
    }
   ],
   "source": [
    "!pip install swifter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and import ontology module from drop-down menu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cdca26d24849dda896dd0f26618a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Files:', options=('2024-8-15_hic_ontology.py', 'scHiCluster_2024-7-5_ontology.py', 'proj…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810e167df7de4b5e88e3c04cdfc704a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Import Selected Ontology Module', layout=Layout(width='auto'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff5f45912004c51a519c0969df70b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_module_name = None\n",
    "imported_module = None\n",
    "\n",
    "def list_files_in_folder(folder_path):\n",
    "    \"\"\"Lists files in the given folder path.\"\"\"\n",
    "    files = [f for f in os.listdir(folder_path) if (os.path.isfile(os.path.join(folder_path, f)) and f.endswith(\"_ontology.py\"))]\n",
    "    return files\n",
    "\n",
    "ontologies_folder = ont_rdb_path + '/ontologies'\n",
    "file_list = list_files_in_folder(ontologies_folder)\n",
    "dropdown_menu = widgets.Dropdown(options=file_list, description='Files:', disabled=False)\n",
    "display(dropdown_menu)\n",
    "\n",
    "button = widgets.Button(description=\"Import Selected Ontology Module\", layout=widgets.Layout(width='auto'))\n",
    "display(button)\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "@button.on_click\n",
    "def button_on_click(b):\n",
    "    global selected_module_name\n",
    "    \n",
    "    global imported_module\n",
    "    with output:\n",
    "        clear_output()\n",
    "        selected_file = dropdown_menu.value\n",
    "        module_name = selected_file[:-3]  # Remove the '.py' extension\n",
    "        import_path = f'ontologies.{module_name}'\n",
    "        \n",
    "        try:\n",
    "            # Dynamically import the selected module\n",
    "            imported_module = importlib.import_module(import_path)\n",
    "            # Optionally, add the imported module to sys.modules\n",
    "            sys.modules[module_name] = imported_module\n",
    "            print(f\"Successfully imported {module_name}.\")\n",
    "            selected_module_name = module_name\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to import {module_name}: {e}.\")\n",
    "\n",
    "display(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ontologies.project_manager_version_1_ontology' from '/home/groups/CEDAR/franksto/ont_rdb/ont_rdb/ontologies/project_manager_version_1_ontology.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imported_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import informant classes from ontology module and construct or access its associated digraph dataframe.\n",
    "If necessary, modify the ``command`` to use appropriate snakemake configurations.\n",
    "\n",
    "For example:\n",
    "* ``use-conda: true``\n",
    "* ``conda-frontend: mamba``\n",
    "* ``cores: 1``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snakemake --profile mamba ontology_dataframes/hic_January_24_2024_ontology_dataframe.pkl\n",
      "Command failed with error: Using profile mamba for setting default command line arguments.\n",
      "Assuming unrestricted shared filesystem usage.\n",
      "Building DAG of jobs...\n",
      "LockException:\n",
      "Error: Directory cannot be locked. Please make sure that no other Snakemake process is trying to create the same files in the following directory:\n",
      "/home/groups/CEDAR/franksto/ont_rdb/ont_rdb\n",
      "If you are sure that no other instances of snakemake are running on this directory, the remaining lock was likely caused by a kill signal or a power loss. It can be removed with the --unlock argument.\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/groups/CEDAR/franksto/ont_rdb/ont_rdb/ontology_dataframes/hic_January_24_2024_ontology_dataframe.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Print the standard error if the command failed\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommand failed with error:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m---> 15\u001b[0m ontology_dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mont_rdb_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/ontology_dataframes/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mselected_module_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_dataframe.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m selected_module \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(selected_module_name)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(selected_module):\n",
      "File \u001b[0;32m/home/groups/CEDAR/franksto/local_envs/ont_rdb_env/lib/python3.12/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/home/groups/CEDAR/franksto/local_envs/ont_rdb_env/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/groups/CEDAR/franksto/ont_rdb/ont_rdb/ontology_dataframes/hic_January_24_2024_ontology_dataframe.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import snakemake\n",
    "command = f\"snakemake --profile {snakemake_profile} ontology_dataframes/{selected_module_name}_dataframe.pkl\"\n",
    "print(command)\n",
    "result = subprocess.run(command, capture_output=True, text=True, shell=True)\n",
    "# Check if the command was successful\n",
    "if result.returncode == 0:\n",
    "    # Print the standard output of the command\n",
    "    print(f\"{selected_module_name} dataframe is constructed.\", result.stdout)\n",
    "else:\n",
    "    # Print the standard error if the command failed\n",
    "    print(\"Command failed with error:\", result.stderr)\n",
    "\n",
    "ontology_dataframe = pd.read_pickle(ont_rdb_path + '/ontology_dataframes/' + selected_module_name + '_dataframe.pkl')\n",
    "\n",
    "selected_module = importlib.import_module(selected_module_name)\n",
    "for name in dir(selected_module):\n",
    "    if not name.startswith('_'):  # Skip internal names\n",
    "        globals()[name] = getattr(selected_module, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore your ontology and create your database.\n",
    "Construct and save informants and informant dataframes to organize  objects in the context of your ontology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ontology_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43montology_dataframe\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ontology_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "ontology_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Most Specific Generalizations (Least Common Ancestors) of Any List of Informant Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque, defaultdict\n",
    "\n",
    "df= ontology_dataframe\n",
    "# Build parent map and depth map\n",
    "parent_map = defaultdict(list)\n",
    "depth_map = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    depth_map[row['informant_subclass_name']] = row['source_depth']\n",
    "    for parent in row['direct_parent_indices']:\n",
    "        parent_map[row['informant_subclass_name']].append(df.iloc[parent]['informant_subclass_name'])\n",
    "\n",
    "# Helper function to find all ancestors\n",
    "# Helper function to find all ancestors\n",
    "def find_ancestors(node, parent_map):\n",
    "    queue = deque([node])\n",
    "    ancestors = set()\n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "        ancestors.add(current)\n",
    "        for parent in parent_map[current]:\n",
    "            if parent not in ancestors:\n",
    "                queue.append(parent)\n",
    "    return ancestors\n",
    "\n",
    "# Helper function to compute unique LCA for two nodes\n",
    "def find_lca_two_nodes(node1, node2, parent_map, depth_map):\n",
    "    queue1 = deque([node1])\n",
    "    queue2 = deque([node2])\n",
    "    \n",
    "    visited1 = set()\n",
    "    visited2 = set()\n",
    "    \n",
    "    while queue1 or queue2:\n",
    "        if queue1:\n",
    "            current1 = queue1.popleft()\n",
    "            if current1 in visited2:\n",
    "                return current1\n",
    "            visited1.add(current1)\n",
    "            for parent in parent_map[current1]:\n",
    "                if parent not in visited1:\n",
    "                    queue1.append(parent)\n",
    "        \n",
    "        if queue2:\n",
    "            current2 = queue2.popleft()\n",
    "            if current2 in visited1:\n",
    "                return current2\n",
    "            visited2.add(current2)\n",
    "            for parent in parent_map[current2]:\n",
    "                if parent not in visited2:\n",
    "                    queue2.append(parent)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Recursive function to compute unique LCA for a list of nodes\n",
    "def find_lca_list(nodes, parent_map, depth_map):\n",
    "    if len(nodes) == 1:\n",
    "        return nodes[0]\n",
    "    \n",
    "    if len(nodes) == 2:\n",
    "        return find_lca_two_nodes(nodes[0], nodes[1], parent_map, depth_map)\n",
    "    \n",
    "    mid = len(nodes) // 2\n",
    "    left_lca = find_lca_list(nodes[:mid], parent_map, depth_map)\n",
    "    right_lca = find_lca_list(nodes[mid:], parent_map, depth_map)\n",
    "    \n",
    "    return find_lca_two_nodes(left_lca, right_lca, parent_map, depth_map)\n",
    "\n",
    "# Find all ancestors of each node at the same depth as the LCA\n",
    "def filter_lcas(lca, nodes, parent_map, depth_map):\n",
    "    lca_depth = depth_map[lca]\n",
    "    all_ancestors = set()\n",
    "    \n",
    "    # Collect all ancestors at the same depth as LCA\n",
    "    for node in nodes:\n",
    "        ancestors = find_ancestors(node, parent_map)\n",
    "        for ancestor in ancestors:\n",
    "            if depth_map[ancestor] == lca_depth:\n",
    "                all_ancestors.add(ancestor)\n",
    "    \n",
    "    # Filter ancestors to retain only those that are common ancestors\n",
    "    valid_lcas = set()\n",
    "    for ancestor in all_ancestors:\n",
    "        if all(ancestor in find_ancestors(node, parent_map) for node in nodes):\n",
    "            valid_lcas.add(ancestor)\n",
    "    \n",
    "    return valid_lcas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest common ancestors of ['ChIP_seq_bigWigAverage_Over_Bed_File', 'HiC_File'] are: {'Computational_Genome_Bio_File'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "nodes = ['ChIP_seq_bigWigAverage_Over_Bed_File', \"HiC_File\"]\n",
    "unique_lca = find_lca_list(nodes, parent_map, depth_map)\n",
    "all_lcas = filter_lcas(unique_lca, nodes, parent_map, depth_map)\n",
    "\n",
    "#print(f'The unique LCA of {nodes} is: {unique_lca}')\n",
    "print(f'The lowest common ancestors of {nodes} are: {all_lcas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty informant dataframe object\n",
    "my_informant_dataframe = informant_class.Informant_Dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>reference_informant_names</th>\n",
       "      <th>informant_class</th>\n",
       "      <th>reference_informant_name_redundancy_values</th>\n",
       "      <th>source_depth</th>\n",
       "      <th>parameter_descriptions</th>\n",
       "      <th>featured_object_type</th>\n",
       "      <th>feature_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>HiC_TAD_Boundary_Caller</td>\n",
       "      <td>{}</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>3D-Genome</td>\n",
       "      <td>TAD_Boundary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name description tags reference_informant_names          informant_class  \\\n",
       "0  None        None   []                        []  HiC_TAD_Boundary_Caller   \n",
       "\n",
       "  reference_informant_name_redundancy_values  source_depth  \\\n",
       "0                                         {}             4   \n",
       "\n",
       "  parameter_descriptions featured_object_type  feature_type  \n",
       "0                   None            3D-Genome  TAD_Boundary  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([HiC_TAD_Boundary_Caller().__dict__])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Bungus',\n",
       " 'description': None,\n",
       " 'tags': [],\n",
       " 'reference_informant_names': [],\n",
       " 'informant_class': 'Parameters',\n",
       " 'reference_informant_name_redundancy_values': {},\n",
       " 'source_depth': 1,\n",
       " 'algorithm': 'TopDom',\n",
       " 'parameter_descriptions': {}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TopDom = TAD_Caller()\n",
    "\n",
    "Parameters(\n",
    "    name='Bungus',\n",
    "    algorithm='TopDom',\n",
    ").__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': None, 'description': None, 'tags': [], 'reference_informant_names': [], 'informant_class': 'BedPe_File', 'reference_informant_name_redundancy_values': {}, 'source_depth': 5, 'species': None, 'location': None, 'external_locations': None, 'file_type': '.bedpe', 'genome_assembly_name': None, 'aliases': None, 'gz': None}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>informant</th>\n",
       "      <th>entry_time</th>\n",
       "      <th>verification_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENCFF661SAZ.bedpe</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.BedPe...</td>\n",
       "      <td>06_28_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                                          informant  \\\n",
       "0  ENCFF661SAZ.bedpe  <ontologies.hic_January_24_2024_ontology.BedPe...   \n",
       "\n",
       "   entry_time verification_status  \n",
       "0  06_28_2024             pending  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize a default BedPe_File informant\n",
    "bedpe_inf = BedPe_File()\n",
    "                           \n",
    "# Use the object's dictionary to see default, characteristic attributes/fields for this class of informant as defined in the ontology.\n",
    "print(bedpe_inf.__dict__)\n",
    "\n",
    "# Populate the fields for this informant by updating its dictionary.\n",
    "bedpe_inf.__dict__.update({'name':\"ENCFF661SAZ.bedpe\",\n",
    "'description': 'Basic loops file from ENCODE.',\n",
    "'species': 'homo_sapiens',\n",
    "'location': \"/home/cfrankston/Projects/Auxiliaries/bedpe_tools/bedpe_data/ENCFF661SAZ.bedpe\",\n",
    "'genome_assembly_name': \"GRCh38\",\n",
    "'gz':False})\n",
    "\n",
    "# Observe that the fields have been populated\n",
    "bedpe_inf.__dict__\n",
    "\n",
    "# Append this informant to the empty informant dataframe\n",
    "my_informant_dataframe.append([bedpe_inf])\n",
    "\n",
    "# Observe that the informant dataframe now contains the informant\n",
    "my_informant_dataframe.df\n",
    "\n",
    "# Test filtering capabilities of the informant dataframe.\n",
    "my_informant_dataframe.filter(\"(@genome_assembly_name == 'GRCh38') & (@gz == False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cfrankston/Projects/Auxiliaries/bedpe_tools/bedpe_data/ENCFF661SAZ.bedpe'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_informant_dataframe.df.iloc[0]['informant'].__dict__['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'KO_VS_Mock.diffloop2', 'description': 'observed_VC_SQRT_5000bp_diff_fdr2_0.05_mustache_fdr1_0.2_results_folder_February_21_2024', 'tags': [], 'reference_informant_names': ['Mustache'], 'informant_class': 'HiC_Loops_File', 'reference_informant_name_redundancy_values': {'Mustache': None}, 'source_depth': 6, 'species': 'homo_sapiens', 'location': '/home/cfrankston/Projects/hic_scope/bedpe_files/KO_VS_Mock.diffloop2', 'external_locations': None, 'file_type': '.bedpe', 'genome_assembly_name': 'GRCh38', 'aliases': None, 'hic_file': None, 'feature_type': 'HiC_Loop', 'gz': False}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>informant</th>\n",
       "      <th>entry_time</th>\n",
       "      <th>verification_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EZH2_KO_Merge.hic</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.HiC_F...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EZH2_Mock_Merge.hic</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.HiC_F...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mzd_setting_1</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.hicst...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KO_VS_Mock.loop2</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.HiC_L...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KO_VS_Mock.diffloop1</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.HiC_L...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KO_VS_Mock.loop1</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.HiC_L...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KO_VS_Mock.diffloop2</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.HiC_L...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KO_VSMock.diffloop1.consensus.bedpe</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.HiC_L...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name  \\\n",
       "0                    EZH2_KO_Merge.hic   \n",
       "1                  EZH2_Mock_Merge.hic   \n",
       "2                        mzd_setting_1   \n",
       "3                     KO_VS_Mock.loop2   \n",
       "4                 KO_VS_Mock.diffloop1   \n",
       "5                     KO_VS_Mock.loop1   \n",
       "6                 KO_VS_Mock.diffloop2   \n",
       "7  KO_VSMock.diffloop1.consensus.bedpe   \n",
       "\n",
       "                                           informant  entry_time  \\\n",
       "0  <ontologies.hic_January_24_2024_ontology.HiC_F...  06_25_2024   \n",
       "1  <ontologies.hic_January_24_2024_ontology.HiC_F...  06_25_2024   \n",
       "2  <ontologies.hic_January_24_2024_ontology.hicst...  06_25_2024   \n",
       "3  <ontologies.hic_January_24_2024_ontology.HiC_L...  06_25_2024   \n",
       "4  <ontologies.hic_January_24_2024_ontology.HiC_L...  06_25_2024   \n",
       "5  <ontologies.hic_January_24_2024_ontology.HiC_L...  06_25_2024   \n",
       "6  <ontologies.hic_January_24_2024_ontology.HiC_L...  06_25_2024   \n",
       "7  <ontologies.hic_January_24_2024_ontology.HiC_L...  06_25_2024   \n",
       "\n",
       "  verification_status  \n",
       "0             pending  \n",
       "1             pending  \n",
       "2             pending  \n",
       "3             pending  \n",
       "4             pending  \n",
       "5             pending  \n",
       "6             pending  \n",
       "7             pending  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_informant_df = informant_class.Informant_Dataframe()\n",
    "\n",
    "informants_list = []\n",
    "informants_list.append(HiC_File(name='EZH2_KO_Merge.hic',\n",
    "                  description='Bulk HiC data from the laboratory of Ted Braun at OHSU of hematopoietic stem cells after an EZH2 CRISPR knockout, merged from three technical replicates by the HiCkory authored by PhD. student Benjamin Skubi in the Yardimci Lab.',\n",
    "                  tags=['Braun_Lab', 'EZH2_Knockout', 'hematopoietic_stem_cell', 'HiCkory'],\n",
    "                  species='homo_sapiens',\n",
    "                  location='/home/cfrankston/Projects/hic_scope/hic_files/KO.hic',\n",
    "                  genome_assembly_name='GRCh38',\n",
    "                  hic_type='in_situ'))\n",
    "\n",
    "informants_list.append(HiC_File(name='EZH2_Mock_Merge.hic',\n",
    "                  description='Bulk HiC data from the laboratory of Ted Braun at OHSU of hematopoietic stem cells controlling against an EZH2 CRISPR knockout, merged from three technical replicates by the HiCkory authored by PhD. student Benjamin Skubi in the Yardimci Lab.',\n",
    "                  tags=['Braun_Lab', 'EZH2_Knockout', 'hematopoietic_stem_cell', 'HiCkory'],\n",
    "                  species='homo_sapiens',\n",
    "                  location='/home/cfrankston/Projects/hic_scope/hic_files/Mock.hic',\n",
    "                  genome_assembly_name='GRCh38',\n",
    "                  hic_type='in_situ'))\n",
    "\n",
    "#print(KO_hic.__dict__)\n",
    "#print('\\n')\n",
    "\n",
    "informants_list.append(hicstraw_getMatrixZoomData_Parameters(name='mzd_setting_1',\n",
    "                                                      parameters={'chr1':1,\n",
    "                                                                  'chr2':1,\n",
    "                                                                  'obs_type':'observed',\n",
    "                                                                  'norm':'VC_SQRT',\n",
    "                                                                  'resolution_units':'BP',\n",
    "                                                                  'res':10000}))\n",
    "#print(my_mzd_params.__dict__)\n",
    "#(hicstraw_getMatrixZoomData.__dict__)\n",
    "\n",
    "Mustache = Algorithm()\n",
    "\n",
    "loop_bedpes_list = informant_class.create_file_informant_list_from_folder(root_folder='/home/cfrankston/Projects/hic_scope/bedpe_files', use_location=True, attribute_sequence=['name'],\n",
    "                                                                              informant_class=HiC_Loops_File, reference_informant_names=['Mustache'], description='observed_VC_SQRT_5000bp_diff_fdr2_0.05_mustache_fdr1_0.2_results_folder_February_21_2024',\n",
    "                                                                              genome_assembly_name='GRCh38', gz=False, species='homo_sapiens')\n",
    "\n",
    "informants_list += (loop_bedpes_list)\n",
    "\n",
    "this_informant_df.append(informants_list)\n",
    "\n",
    "this_informant_df.append([HiC_Loops_File(name='KO_VSMock.diffloop1.consensus.bedpe', description='Preliminary consensus loops produced between two Mustache loop calls at different normalizations and fdr rates and an arbitrary consensus score threshold at 10kbp resolution.', reference_informant_names=['Mustache'], tags=['consensus_features', 'EZH2_KO', 'hematopoietic_stem_cells'], genome_assembly_name='GRCh38', gz=False, species='homo_sapiens', location = '/home/cfrankston/Projects/consensus_features/consensus_features/results/KO_VS_Mock.diffloop1.consensus.bedpe')])\n",
    "\n",
    "print(this_informant_df.df.loc[6]['informant'].__dict__)\n",
    "this_informant_df.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n",
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n",
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n",
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n",
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n",
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n",
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n",
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n"
     ]
    }
   ],
   "source": [
    "this_informant_df.df['verification_status'] = True\n",
    "this_informant_df.filter('isinstance(@informant, HiC_Loops_File)', additional_context={'HiC_Loops_File':HiC_Loops_File})\n",
    "\n",
    "this_informant_df.save_df(df_pkl_path='/home/cfrankston/Projects/hic_scope/informant_dataframes/hic_scope_test_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_trial_informant_df = informant_class.Informant_Dataframe()\n",
    "\n",
    "putative_loops_inf_df = informant_class.create_file_informant_list_from_folder(root_folder = \"/home/cfrankston/Projects/CEDAR_Projects/2024-02-21_EZH2-knockout-hic/data/data_March_13_2024\",\n",
    "use_location=True, attribute_sequence=['description','name'],informant_class=HiC_Loops_File, reference_informant_names=['Mustache'],\n",
    "                                                                              genome_assembly_name='GRCh38', gz=False, species='homo_sapiens')\n",
    "\n",
    "consensus_trial_informant_df.append(putative_loops_inf_df)\n",
    "consensus_trial_informant_df.df\n",
    "\n",
    "consensus_trial_informant_df.save_df(df_pkl_path='/home/cfrankston/Projects/consensus_features/consensus_features/informant_dataframes/loop_rep_infs_df')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'rep1_KO_VS_Mock.diffloop2',\n",
       " 'description': 'observed_KR_10000bp_diff_fdr2_0.05_mustache_fdr1_0.15_results_folder_March_7_2024',\n",
       " 'tags': [],\n",
       " 'reference_informant_names': ['Mustache'],\n",
       " 'informant_class': 'HiC_Loops_File',\n",
       " 'reference_informant_name_redundancy_values': {'Mustache': None},\n",
       " 'source_depth': 6,\n",
       " 'species': 'homo_sapiens',\n",
       " 'location': '/home/cfrankston/Projects/CEDAR_Projects/2024-02-21_EZH2-knockout-hic/data/observed_KR_10000bp_diff_fdr2_0.05_mustache_fdr1_0.15_results_folder_March_7_2024/KO_VS_Mock/rep1_KO_VS_Mock.diffloop2',\n",
       " 'external_locations': None,\n",
       " 'file_type': '.bedpe',\n",
       " 'genome_assembly_name': 'GRCh38',\n",
       " 'aliases': None,\n",
       " 'hic_file': None,\n",
       " 'feature_type': 'HiC_Loop',\n",
       " 'gz': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consensus_trial_informant_df.df['informant'][0].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Project using Imported Ontology and Desired Informant Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing command: \n",
      "python  launch_project.py project_manager/version_1.0 /home/groups/CEDAR/franksto/ont_rdb/ont_rdb/informant_class.py /home/groups/CEDAR/franksto/ont_rdb/ont_rdb/ontologies/project_manager_version_1_ontology.py ont_rdb/ont_rdb/ontology_dataframes/hic_January_24_2024_ontology_dataframe.pkl /home/groups/CEDAR/franksto\n",
      "Created project structure.\n",
      "Linked informant_class script, ontology and informants.\n",
      "Created metadata file.\n",
      "Copied consolidate_project.py.\n",
      "Creating explorer notebook for project_manager/version_1.0 in /home/groups/CEDAR/franksto/project_manager/version_1.0.\n",
      "Created explorer notebook.\n",
      "\n",
      "Project named project_manager/version_1.0 is constructed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import snakemake\n",
    "\n",
    "project_name = \"project_manager/version_1.0\"#\"transcription-from-3D-genome_attributions/version_1.0\"\n",
    "informant_class_path = \"/home/groups/CEDAR/franksto/ont_rdb/ont_rdb/informant_class.py\"\n",
    "ontology_script_path = \"/home/groups/CEDAR/franksto/ont_rdb/ont_rdb/ontologies/project_manager_version_1_ontology.py\"\n",
    "informant_dataframe_path = \"ont_rdb/ont_rdb/ontology_dataframes/hic_January_24_2024_ontology_dataframe.pkl\"\n",
    "base_directory = \"/home/groups/CEDAR/franksto\"#/2024-6-24\"\n",
    "\n",
    "meta_log = \"\"\n",
    "\n",
    "command = f\"python  launch_project_2.0.py {project_name} {informant_class_path} {ontology_script_path} {informant_dataframe_path} {base_directory}\"\n",
    "result = subprocess.run(command, capture_output=True, text=True, shell=True)\n",
    "\n",
    "print(f\"Processing command: \\n{command}\")\n",
    "# Check if the command was successful\n",
    "if result.returncode == 0:\n",
    "    # Print the standard output of the command\n",
    "    print(result.stdout)\n",
    "    print(f\"Project named {project_name} is constructed.\")\n",
    "else:\n",
    "    # Print the standard error if the command failed\n",
    "    print(\"Command failed with error:\", result.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ont_rdb_env]",
   "language": "python",
   "name": "conda-env-ont_rdb_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
