{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ont_rdb_explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "Also set the path to ont_rdb and the name of your snakemake profile for processing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T20:50:16.258870Z",
     "iopub.status.busy": "2025-03-13T20:50:16.257674Z"
    }
   },
   "outputs": [],
   "source": [
    "#import informant_class\n",
    "#import swifter\n",
    "from informant_class import *\n",
    "# Adjust this to the directory storing the ont_rdb package.\n",
    "ont_rdb_path = \"/home/groups/CEDAR/franksto/ont_rdb/ont_rdb\"\n",
    "# Set the snakemake profile you wish to use:\n",
    "snakemake_profile = 'mamba'\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "import importlib\n",
    "import sys\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and import ontology module from drop-down menu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_module_name = None\n",
    "imported_module = None\n",
    "\n",
    "def list_files_in_folder(folder_path):\n",
    "    \"\"\"Lists files in the given folder path.\"\"\"\n",
    "    files = [f for f in os.listdir(folder_path) if (os.path.isfile(os.path.join(folder_path, f)) and f.endswith(\"_ontology.py\"))]\n",
    "    return files\n",
    "\n",
    "ontologies_folder = ont_rdb_path + '/ontologies'\n",
    "file_list = list_files_in_folder(ontologies_folder)\n",
    "dropdown_menu = widgets.Dropdown(options=file_list, description='Files:', disabled=False)\n",
    "display(dropdown_menu)\n",
    "\n",
    "button = widgets.Button(description=\"Import Selected Ontology Module\", layout=widgets.Layout(width='auto'))\n",
    "display(button)\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "@button.on_click\n",
    "def button_on_click(b):\n",
    "    global selected_module_name\n",
    "    \n",
    "    global imported_module\n",
    "    with output:\n",
    "        clear_output()\n",
    "        selected_file = dropdown_menu.value\n",
    "        module_name = selected_file[:-3]  # Remove the '.py' extension\n",
    "        import_path = f'ontologies.{module_name}'\n",
    "        \n",
    "        try:\n",
    "            # Dynamically import the selected module\n",
    "            imported_module = importlib.import_module(import_path)\n",
    "            # Optionally, add the imported module to sys.modules\n",
    "            sys.modules[module_name] = imported_module\n",
    "            print(f\"Successfully imported {module_name}.\")\n",
    "            selected_module_name = module_name\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to import {module_name}: {e}.\")\n",
    "\n",
    "display(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ontologies.project_manager_version_1_ontology' from '/home/groups/CEDAR/franksto/ont_rdb/ont_rdb/ontologies/project_manager_version_1_ontology.py'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imported_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import informant classes from ontology module and construct or access its associated digraph dataframe.\n",
    "If necessary, modify the ``command`` to use appropriate snakemake configurations.\n",
    "\n",
    "For example:\n",
    "* ``use-conda: true``\n",
    "* ``conda-frontend: mamba``\n",
    "* ``cores: 1``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîì Unlocking Snakemake: snakemake --profile mamba --unlock\n",
      "‚úÖ Snakemake unlocked successfully.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run Snakemake unlock command\n",
    "unlock_command = f\"snakemake --profile {snakemake_profile} --unlock\"\n",
    "print(f\"üîì Unlocking Snakemake: {unlock_command}\")\n",
    "unlock_result = subprocess.run(unlock_command, shell=True, capture_output=True, text=True)\n",
    "\n",
    "# Check if unlock was successful\n",
    "if unlock_result.returncode == 0:\n",
    "    print(\"‚úÖ Snakemake unlocked successfully.\")\n",
    "else:\n",
    "    print(f\"‚ùå Unlock failed. Error:\\n{unlock_result.stderr}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: snakemake --profile mamba ontology_dataframes/2024-10-14_hic_vae_ontology_dataframe.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using profile mamba for setting default command line arguments.\n",
      "Assuming unrestricted shared filesystem usage.\n",
      "Building DAG of jobs...\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import snakemake\n",
    "command = f\"snakemake --profile {snakemake_profile} ontology_dataframes/{selected_module_name}_dataframe.pkl\"\n",
    "\n",
    "print(f\"Executing: {command}\")\n",
    "result = subprocess.run(command, shell=True)\n",
    "#result = subprocess.run(command, capture_output=True, text=True, shell=True)\n",
    "\n",
    "# Check if Snakemake succeeded\n",
    "if result.returncode == 0:\n",
    "    print(f\"\\n‚úÖ {selected_module_name} dataframe successfully constructed.\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Snakemake failed. Check {log_file} for details.\\nError:\\n\", result.stderr)\n",
    "    exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_dataframe = pd.read_pickle(ont_rdb_path + '/ontology_dataframes/' + selected_module_name + '_dataframe.pkl')\n",
    "\n",
    "selected_module = importlib.import_module(selected_module_name)\n",
    "for name in dir(selected_module):\n",
    "    if not name.startswith('_'):  # Skip internal names\n",
    "        globals()[name] = getattr(selected_module, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore your ontology and create your database.\n",
    "Construct and save informants and informant dataframes to organize  objects in the context of your ontology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>informant_subclass_name</th>\n",
       "      <th>informant_subclass</th>\n",
       "      <th>direct_parent_indices</th>\n",
       "      <th>direct_child_indices</th>\n",
       "      <th>is_sink</th>\n",
       "      <th>source_depth</th>\n",
       "      <th>sink_depth</th>\n",
       "      <th>to_nearest_sink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Informant</td>\n",
       "      <td>&lt;class 'informant_class.Informant'&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>[1, 3, 4, 5, 6, 8, 9, 10, 11, 17]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[3, 4, 5, 6, 9, 11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Directory_Informant</td>\n",
       "      <td>&lt;class 'informant_class.Directory_Informant'&gt;</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[2, 7, 18, 19]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[7, 18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>File_Informant</td>\n",
       "      <td>&lt;class 'informant_class.File_Informant'&gt;</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[15, 16, 20, 25]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[15, 16, 25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Informant_Dataframe</td>\n",
       "      <td>&lt;class 'informant_class.Informant_Dataframe'&gt;</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DataBase</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Algorithm</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Parameters</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>File_Set</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Institution</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[12, 13]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[12, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Article</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Website</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[14]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Laboratory</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Company</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>University</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Github_Repository</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[10]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ontology_Script</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Algorithm_Script</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Project</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[18]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Project_Build</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[17, 1]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Log</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[23]</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Log_Entry</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[21]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Project_Log_Entry</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[20]</td>\n",
       "      <td>[22]</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Project_Build_Log_Entry</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[21]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Project_Log</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[19]</td>\n",
       "      <td>[24]</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Project_Build_Log</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[23]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Compiled_Log</td>\n",
       "      <td>&lt;class 'ontologies.project_manager_version_1_o...</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    informant_subclass_name  \\\n",
       "0                 Informant   \n",
       "1       Directory_Informant   \n",
       "2            File_Informant   \n",
       "3       Informant_Dataframe   \n",
       "4                  DataBase   \n",
       "5                 Algorithm   \n",
       "6                Parameters   \n",
       "7                  File_Set   \n",
       "8               Institution   \n",
       "9                   Article   \n",
       "10                  Website   \n",
       "11               Laboratory   \n",
       "12                  Company   \n",
       "13               University   \n",
       "14        Github_Repository   \n",
       "15          Ontology_Script   \n",
       "16         Algorithm_Script   \n",
       "17                  Project   \n",
       "18            Project_Build   \n",
       "19                      Log   \n",
       "20                Log_Entry   \n",
       "21        Project_Log_Entry   \n",
       "22  Project_Build_Log_Entry   \n",
       "23              Project_Log   \n",
       "24        Project_Build_Log   \n",
       "25             Compiled_Log   \n",
       "\n",
       "                                   informant_subclass direct_parent_indices  \\\n",
       "0                 <class 'informant_class.Informant'>                    []   \n",
       "1       <class 'informant_class.Directory_Informant'>                   [0]   \n",
       "2            <class 'informant_class.File_Informant'>                   [1]   \n",
       "3       <class 'informant_class.Informant_Dataframe'>                   [0]   \n",
       "4   <class 'ontologies.project_manager_version_1_o...                   [0]   \n",
       "5   <class 'ontologies.project_manager_version_1_o...                   [0]   \n",
       "6   <class 'ontologies.project_manager_version_1_o...                   [0]   \n",
       "7   <class 'ontologies.project_manager_version_1_o...                   [1]   \n",
       "8   <class 'ontologies.project_manager_version_1_o...                   [0]   \n",
       "9   <class 'ontologies.project_manager_version_1_o...                   [0]   \n",
       "10  <class 'ontologies.project_manager_version_1_o...                   [0]   \n",
       "11  <class 'ontologies.project_manager_version_1_o...                   [0]   \n",
       "12  <class 'ontologies.project_manager_version_1_o...                   [8]   \n",
       "13  <class 'ontologies.project_manager_version_1_o...                   [8]   \n",
       "14  <class 'ontologies.project_manager_version_1_o...                  [10]   \n",
       "15  <class 'ontologies.project_manager_version_1_o...                   [2]   \n",
       "16  <class 'ontologies.project_manager_version_1_o...                   [2]   \n",
       "17  <class 'ontologies.project_manager_version_1_o...                   [0]   \n",
       "18  <class 'ontologies.project_manager_version_1_o...               [17, 1]   \n",
       "19  <class 'ontologies.project_manager_version_1_o...                   [1]   \n",
       "20  <class 'ontologies.project_manager_version_1_o...                   [2]   \n",
       "21  <class 'ontologies.project_manager_version_1_o...                  [20]   \n",
       "22  <class 'ontologies.project_manager_version_1_o...                  [21]   \n",
       "23  <class 'ontologies.project_manager_version_1_o...                  [19]   \n",
       "24  <class 'ontologies.project_manager_version_1_o...                  [23]   \n",
       "25  <class 'ontologies.project_manager_version_1_o...                   [2]   \n",
       "\n",
       "                 direct_child_indices  is_sink  source_depth  sink_depth  \\\n",
       "0   [1, 3, 4, 5, 6, 8, 9, 10, 11, 17]        0             0           1   \n",
       "1                      [2, 7, 18, 19]        0             1           1   \n",
       "2                    [15, 16, 20, 25]        0             2           1   \n",
       "3                                  []        1             1           0   \n",
       "4                                  []        1             1           0   \n",
       "5                                  []        1             1           0   \n",
       "6                                  []        1             1           0   \n",
       "7                                  []        1             2           0   \n",
       "8                            [12, 13]        0             1           1   \n",
       "9                                  []        1             1           0   \n",
       "10                               [14]        0             1           1   \n",
       "11                                 []        1             1           0   \n",
       "12                                 []        1             2           0   \n",
       "13                                 []        1             2           0   \n",
       "14                                 []        1             2           0   \n",
       "15                                 []        1             3           0   \n",
       "16                                 []        1             3           0   \n",
       "17                               [18]        0             1           1   \n",
       "18                                 []        1             2           0   \n",
       "19                               [23]        0             2           2   \n",
       "20                               [21]        0             3           2   \n",
       "21                               [22]        0             4           1   \n",
       "22                                 []        1             5           0   \n",
       "23                               [24]        0             3           1   \n",
       "24                                 []        1             4           0   \n",
       "25                                 []        1             3           0   \n",
       "\n",
       "        to_nearest_sink  \n",
       "0   [3, 4, 5, 6, 9, 11]  \n",
       "1               [7, 18]  \n",
       "2          [15, 16, 25]  \n",
       "3                    []  \n",
       "4                    []  \n",
       "5                    []  \n",
       "6                    []  \n",
       "7                    []  \n",
       "8              [12, 13]  \n",
       "9                    []  \n",
       "10                 [14]  \n",
       "11                   []  \n",
       "12                   []  \n",
       "13                   []  \n",
       "14                   []  \n",
       "15                   []  \n",
       "16                   []  \n",
       "17                 [18]  \n",
       "18                   []  \n",
       "19                 [23]  \n",
       "20                 [21]  \n",
       "21                 [22]  \n",
       "22                   []  \n",
       "23                 [24]  \n",
       "24                   []  \n",
       "25                   []  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ontology_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install networkx\n",
    "#!pip install pyvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "‚úÖ Interactive ontology graph saved to project_manager_version_1_ontology_graph.html\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load ontology dataframe\n",
    "ontology_df = ontology_dataframe  # Assuming it's already loaded\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes with labels\n",
    "for _, row in ontology_df.iterrows():\n",
    "    node = row['informant_subclass_name']\n",
    "    G.add_node(node, title=node)  # title is used for hover tooltip\n",
    "\n",
    "# Add edges based on parent-child relationships\n",
    "for _, row in ontology_df.iterrows():\n",
    "    parent_indices = row['direct_parent_indices']\n",
    "    child = row['informant_subclass_name']\n",
    "    for parent_index in parent_indices:\n",
    "        parent = ontology_df.iloc[parent_index]['informant_subclass_name']\n",
    "        G.add_edge(parent, child)\n",
    "\n",
    "# Create an interactive visualization using pyvis\n",
    "net = Network(height=\"750px\", width=\"100%\", directed=True, notebook=True)\n",
    "\n",
    "# Convert NetworkX graph to Pyvis\n",
    "net.from_nx(G)\n",
    "\n",
    "# Customize node appearance\n",
    "for node in net.nodes:\n",
    "    node['size'] = G.out_degree(node['id']) * 3  # Scale by number of children\n",
    "    node['color'] = 'red' if G.out_degree(node['id']) > 5 else 'blue'  # Highlight hubs\n",
    "    node['title'] = f\"{node['id']} (Children: {G.out_degree(node['id'])})\"\n",
    "\n",
    "# Save and open the interactive visualization\n",
    "output_path = f\"{selected_module_name}_graph.html\"\n",
    "net.save_graph(output_path)\n",
    "print(f\"‚úÖ Interactive ontology graph saved to {output_path}\")\n",
    "\n",
    "# Open in browser automatically (only works if running locally)\n",
    "if os.path.exists(output_path):\n",
    "    import webbrowser\n",
    "    webbrowser.open(f\"file://{os.path.abspath(output_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Interactive dependency graph saved to dependency_graph.html\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def extract_functions(script_content):\n",
    "    \"\"\"Extracts function names from a Python script.\"\"\"\n",
    "    function_pattern = re.compile(r'def (\\w+)\\(')\n",
    "    return function_pattern.findall(script_content)\n",
    "\n",
    "def extract_function_calls(script_content, function_names):\n",
    "    \"\"\"Extracts function calls from a Python script, limited to known functions.\"\"\"\n",
    "    call_pattern = re.compile(r'(\\w+)\\(')\n",
    "    calls = call_pattern.findall(script_content)\n",
    "    return [call for call in calls if call in function_names]\n",
    "\n",
    "def build_dependency_graph(script_path):\n",
    "    \"\"\"Builds a function dependency graph from a Python script.\"\"\"\n",
    "    if not os.path.exists(script_path):\n",
    "        print(f\"‚ùå Error: The script '{script_path}' does not exist.\")\n",
    "        return None\n",
    "\n",
    "    with open(script_path, 'r', encoding='utf-8') as f:\n",
    "        script_content = f.read()\n",
    "\n",
    "    function_names = extract_functions(script_content)\n",
    "\n",
    "    # Build the graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for func in function_names:\n",
    "        G.add_node(func)\n",
    "\n",
    "    function_blocks = re.split(r'def (\\w+)\\(', script_content)[1:]  # Split by function definitions\n",
    "    for i in range(0, len(function_blocks), 2):\n",
    "        func_name = function_blocks[i]\n",
    "        func_body = function_blocks[i + 1] if i + 1 < len(function_blocks) else \"\"\n",
    "        called_functions = extract_function_calls(func_body, function_names)\n",
    "        for called_func in called_functions:\n",
    "            G.add_edge(func_name, called_func)\n",
    "\n",
    "    return G\n",
    "\n",
    "def generate_interactive_graph(G, output_path=\"dependency_graph.html\"):\n",
    "    \"\"\"Generates an interactive dependency graph using pyvis.\"\"\"\n",
    "    if G is None or len(G.nodes) == 0:\n",
    "        print(\"‚ö†Ô∏è No functions detected. Ensure your script has function definitions.\")\n",
    "        return\n",
    "\n",
    "    net = Network(height=\"750px\", width=\"100%\", directed=True, notebook=False)\n",
    "    net.from_nx(G)\n",
    "\n",
    "    # Customize node appearance\n",
    "    for node in net.nodes:\n",
    "        node['size'] = G.out_degree(node['id']) * 5  # Scale by number of calls\n",
    "        node['color'] = 'red' if G.out_degree(node['id']) > 5 else 'blue'\n",
    "        node['title'] = f\"{node['id']} (Calls: {G.out_degree(node['id'])})\"\n",
    "\n",
    "    # Save and open the interactive visualization\n",
    "    net.save_graph(output_path)\n",
    "    print(f\"‚úÖ Interactive dependency graph saved to {output_path}\")\n",
    "\n",
    "    # Open in browser (only if running locally)\n",
    "    if os.path.exists(output_path):\n",
    "        import webbrowser\n",
    "        webbrowser.open(f\"file://{os.path.abspath(output_path)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python dependency_graph.py <path_to_python_script>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    script_path = '/home/groups/CEDAR/franksto/ont_rdb/ont_rdb/informant_class.py'#sys.argv[1]\n",
    "    G = build_dependency_graph(script_path)\n",
    "    generate_interactive_graph(G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Most Specific Generalizations (Least Common Ancestors) of Any List of Informant Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, defaultdict\n",
    "\n",
    "df= ontology_dataframe\n",
    "# Build parent map and depth map\n",
    "parent_map = defaultdict(list)\n",
    "depth_map = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    depth_map[row['informant_subclass_name']] = row['source_depth']\n",
    "    for parent in row['direct_parent_indices']:\n",
    "        parent_map[row['informant_subclass_name']].append(df.iloc[parent]['informant_subclass_name'])\n",
    "\n",
    "# Helper function to find all ancestors\n",
    "# Helper function to find all ancestors\n",
    "def find_ancestors(node, parent_map):\n",
    "    queue = deque([node])\n",
    "    ancestors = set()\n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "        ancestors.add(current)\n",
    "        for parent in parent_map[current]:\n",
    "            if parent not in ancestors:\n",
    "                queue.append(parent)\n",
    "    return ancestors\n",
    "\n",
    "# Helper function to compute unique LCA for two nodes\n",
    "def find_lca_two_nodes(node1, node2, parent_map, depth_map):\n",
    "    queue1 = deque([node1])\n",
    "    queue2 = deque([node2])\n",
    "    \n",
    "    visited1 = set()\n",
    "    visited2 = set()\n",
    "    \n",
    "    while queue1 or queue2:\n",
    "        if queue1:\n",
    "            current1 = queue1.popleft()\n",
    "            if current1 in visited2:\n",
    "                return current1\n",
    "            visited1.add(current1)\n",
    "            for parent in parent_map[current1]:\n",
    "                if parent not in visited1:\n",
    "                    queue1.append(parent)\n",
    "        \n",
    "        if queue2:\n",
    "            current2 = queue2.popleft()\n",
    "            if current2 in visited1:\n",
    "                return current2\n",
    "            visited2.add(current2)\n",
    "            for parent in parent_map[current2]:\n",
    "                if parent not in visited2:\n",
    "                    queue2.append(parent)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Recursive function to compute unique LCA for a list of nodes\n",
    "def find_lca_list(nodes, parent_map, depth_map):\n",
    "    if len(nodes) == 1:\n",
    "        return nodes[0]\n",
    "    \n",
    "    if len(nodes) == 2:\n",
    "        return find_lca_two_nodes(nodes[0], nodes[1], parent_map, depth_map)\n",
    "    \n",
    "    mid = len(nodes) // 2\n",
    "    left_lca = find_lca_list(nodes[:mid], parent_map, depth_map)\n",
    "    right_lca = find_lca_list(nodes[mid:], parent_map, depth_map)\n",
    "    \n",
    "    return find_lca_two_nodes(left_lca, right_lca, parent_map, depth_map)\n",
    "\n",
    "# Find all ancestors of each node at the same depth as the LCA\n",
    "def filter_lcas(lca, nodes, parent_map, depth_map):\n",
    "    lca_depth = depth_map[lca]\n",
    "    all_ancestors = set()\n",
    "    \n",
    "    # Collect all ancestors at the same depth as LCA\n",
    "    for node in nodes:\n",
    "        ancestors = find_ancestors(node, parent_map)\n",
    "        for ancestor in ancestors:\n",
    "            if depth_map[ancestor] == lca_depth:\n",
    "                all_ancestors.add(ancestor)\n",
    "    \n",
    "    # Filter ancestors to retain only those that are common ancestors\n",
    "    valid_lcas = set()\n",
    "    for ancestor in all_ancestors:\n",
    "        if all(ancestor in find_ancestors(node, parent_map) for node in nodes):\n",
    "            valid_lcas.add(ancestor)\n",
    "    \n",
    "    return valid_lcas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lowest common ancestors of ['ChIP_seq_bigWigAverage_Over_Bed_File', 'Bed_File', 'bigWig_File', 'HiC_File', 'Genome_Assembly'] are: {'Computational_Bio_Source'}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "nodes = ['ChIP_seq_bigWigAverage_Over_Bed_File',\"Bed_File\", \"bigWig_File\", \"HiC_File\", \"Genome_Assembly\"]\n",
    "unique_lca = find_lca_list(nodes, parent_map, depth_map)\n",
    "all_lcas = filter_lcas(unique_lca, nodes, parent_map, depth_map)\n",
    "\n",
    "#print(f'The unique LCA of {nodes} is: {unique_lca}')\n",
    "print(f'The lowest common ancestors of {nodes} are: {all_lcas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty informant dataframe object\n",
    "my_informant_dataframe = informant_class.Informant_Dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "      <th>reference_informant_names</th>\n",
       "      <th>informant_class</th>\n",
       "      <th>reference_informant_name_redundancy_values</th>\n",
       "      <th>source_depth</th>\n",
       "      <th>parameter_descriptions</th>\n",
       "      <th>featured_object_type</th>\n",
       "      <th>feature_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>HiC_TAD_Boundary_Caller</td>\n",
       "      <td>{}</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>3D-Genome</td>\n",
       "      <td>TAD_Boundary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name description tags reference_informant_names          informant_class  \\\n",
       "0  None        None   []                        []  HiC_TAD_Boundary_Caller   \n",
       "\n",
       "  reference_informant_name_redundancy_values  source_depth  \\\n",
       "0                                         {}             4   \n",
       "\n",
       "  parameter_descriptions featured_object_type  feature_type  \n",
       "0                   None            3D-Genome  TAD_Boundary  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([HiC_TAD_Boundary_Caller().__dict__])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Bungus',\n",
       " 'description': None,\n",
       " 'tags': [],\n",
       " 'reference_informant_names': [],\n",
       " 'informant_class': 'Parameters',\n",
       " 'reference_informant_name_redundancy_values': {},\n",
       " 'source_depth': 1,\n",
       " 'algorithm': 'TopDom',\n",
       " 'parameter_descriptions': {}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TopDom = TAD_Caller()\n",
    "\n",
    "Parameters(\n",
    "    name='Bungus',\n",
    "    algorithm='TopDom',\n",
    ").__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': None, 'description': None, 'tags': [], 'reference_informant_names': [], 'informant_class': 'BedPe_File', 'reference_informant_name_redundancy_values': {}, 'source_depth': 5, 'species': None, 'location': None, 'external_locations': None, 'file_type': '.bedpe', 'genome_assembly_name': None, 'aliases': None, 'gz': None}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>informant</th>\n",
       "      <th>entry_time</th>\n",
       "      <th>verification_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENCFF661SAZ.bedpe</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.BedPe...</td>\n",
       "      <td>06_28_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                                          informant  \\\n",
       "0  ENCFF661SAZ.bedpe  <ontologies.hic_January_24_2024_ontology.BedPe...   \n",
       "\n",
       "   entry_time verification_status  \n",
       "0  06_28_2024             pending  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize a default BedPe_File informant\n",
    "bedpe_inf = BedPe_File()\n",
    "                           \n",
    "# Use the object's dictionary to see default, characteristic attributes/fields for this class of informant as defined in the ontology.\n",
    "print(bedpe_inf.__dict__)\n",
    "\n",
    "# Populate the fields for this informant by updating its dictionary.\n",
    "bedpe_inf.__dict__.update({'name':\"ENCFF661SAZ.bedpe\",\n",
    "'description': 'Basic loops file from ENCODE.',\n",
    "'species': 'homo_sapiens',\n",
    "'location': \"/home/cfrankston/Projects/Auxiliaries/bedpe_tools/bedpe_data/ENCFF661SAZ.bedpe\",\n",
    "'genome_assembly_name': \"GRCh38\",\n",
    "'gz':False})\n",
    "\n",
    "# Observe that the fields have been populated\n",
    "bedpe_inf.__dict__\n",
    "\n",
    "# Append this informant to the empty informant dataframe\n",
    "my_informant_dataframe.append([bedpe_inf])\n",
    "\n",
    "# Observe that the informant dataframe now contains the informant\n",
    "my_informant_dataframe.df\n",
    "\n",
    "# Test filtering capabilities of the informant dataframe.\n",
    "my_informant_dataframe.filter(\"(@genome_assembly_name == 'GRCh38') & (@gz == False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cfrankston/Projects/Auxiliaries/bedpe_tools/bedpe_data/ENCFF661SAZ.bedpe'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_informant_dataframe.df.iloc[0]['informant'].__dict__['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'KO_VS_Mock.diffloop2', 'description': 'observed_VC_SQRT_5000bp_diff_fdr2_0.05_mustache_fdr1_0.2_results_folder_February_21_2024', 'tags': [], 'reference_informant_names': ['Mustache'], 'informant_class': 'HiC_Loops_File', 'reference_informant_name_redundancy_values': {'Mustache': None}, 'source_depth': 6, 'species': 'homo_sapiens', 'location': '/home/cfrankston/Projects/hic_scope/bedpe_files/KO_VS_Mock.diffloop2', 'external_locations': None, 'file_type': '.bedpe', 'genome_assembly_name': 'GRCh38', 'aliases': None, 'hic_file': None, 'feature_type': 'HiC_Loop', 'gz': False}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>informant</th>\n",
       "      <th>entry_time</th>\n",
       "      <th>verification_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EZH2_KO_Merge.hic</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.HiC_F...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EZH2_Mock_Merge.hic</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.HiC_F...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mzd_setting_1</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.hicst...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KO_VS_Mock.loop2</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.HiC_L...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KO_VS_Mock.diffloop1</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.HiC_L...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KO_VS_Mock.loop1</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.HiC_L...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KO_VS_Mock.diffloop2</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.HiC_L...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KO_VSMock.diffloop1.consensus.bedpe</td>\n",
       "      <td>&lt;ontologies.hic_January_24_2024_ontology.HiC_L...</td>\n",
       "      <td>06_25_2024</td>\n",
       "      <td>pending</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name  \\\n",
       "0                    EZH2_KO_Merge.hic   \n",
       "1                  EZH2_Mock_Merge.hic   \n",
       "2                        mzd_setting_1   \n",
       "3                     KO_VS_Mock.loop2   \n",
       "4                 KO_VS_Mock.diffloop1   \n",
       "5                     KO_VS_Mock.loop1   \n",
       "6                 KO_VS_Mock.diffloop2   \n",
       "7  KO_VSMock.diffloop1.consensus.bedpe   \n",
       "\n",
       "                                           informant  entry_time  \\\n",
       "0  <ontologies.hic_January_24_2024_ontology.HiC_F...  06_25_2024   \n",
       "1  <ontologies.hic_January_24_2024_ontology.HiC_F...  06_25_2024   \n",
       "2  <ontologies.hic_January_24_2024_ontology.hicst...  06_25_2024   \n",
       "3  <ontologies.hic_January_24_2024_ontology.HiC_L...  06_25_2024   \n",
       "4  <ontologies.hic_January_24_2024_ontology.HiC_L...  06_25_2024   \n",
       "5  <ontologies.hic_January_24_2024_ontology.HiC_L...  06_25_2024   \n",
       "6  <ontologies.hic_January_24_2024_ontology.HiC_L...  06_25_2024   \n",
       "7  <ontologies.hic_January_24_2024_ontology.HiC_L...  06_25_2024   \n",
       "\n",
       "  verification_status  \n",
       "0             pending  \n",
       "1             pending  \n",
       "2             pending  \n",
       "3             pending  \n",
       "4             pending  \n",
       "5             pending  \n",
       "6             pending  \n",
       "7             pending  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_informant_df = informant_class.Informant_Dataframe()\n",
    "\n",
    "informants_list = []\n",
    "informants_list.append(HiC_File(name='EZH2_KO_Merge.hic',\n",
    "                  description='Bulk HiC data from the laboratory of Ted Braun at OHSU of hematopoietic stem cells after an EZH2 CRISPR knockout, merged from three technical replicates by the HiCkory authored by PhD. student Benjamin Skubi in the Yardimci Lab.',\n",
    "                  tags=['Braun_Lab', 'EZH2_Knockout', 'hematopoietic_stem_cell', 'HiCkory'],\n",
    "                  species='homo_sapiens',\n",
    "                  location='/home/cfrankston/Projects/hic_scope/hic_files/KO.hic',\n",
    "                  genome_assembly_name='GRCh38',\n",
    "                  hic_type='in_situ'))\n",
    "\n",
    "informants_list.append(HiC_File(name='EZH2_Mock_Merge.hic',\n",
    "                  description='Bulk HiC data from the laboratory of Ted Braun at OHSU of hematopoietic stem cells controlling against an EZH2 CRISPR knockout, merged from three technical replicates by the HiCkory authored by PhD. student Benjamin Skubi in the Yardimci Lab.',\n",
    "                  tags=['Braun_Lab', 'EZH2_Knockout', 'hematopoietic_stem_cell', 'HiCkory'],\n",
    "                  species='homo_sapiens',\n",
    "                  location='/home/cfrankston/Projects/hic_scope/hic_files/Mock.hic',\n",
    "                  genome_assembly_name='GRCh38',\n",
    "                  hic_type='in_situ'))\n",
    "\n",
    "#print(KO_hic.__dict__)\n",
    "#print('\\n')\n",
    "\n",
    "informants_list.append(hicstraw_getMatrixZoomData_Parameters(name='mzd_setting_1',\n",
    "                                                      parameters={'chr1':1,\n",
    "                                                                  'chr2':1,\n",
    "                                                                  'obs_type':'observed',\n",
    "                                                                  'norm':'VC_SQRT',\n",
    "                                                                  'resolution_units':'BP',\n",
    "                                                                  'res':10000}))\n",
    "#print(my_mzd_params.__dict__)\n",
    "#(hicstraw_getMatrixZoomData.__dict__)\n",
    "\n",
    "Mustache = Algorithm()\n",
    "\n",
    "loop_bedpes_list = informant_class.create_file_informant_list_from_folder(root_folder='/home/cfrankston/Projects/hic_scope/bedpe_files', use_location=True, attribute_sequence=['name'],\n",
    "                                                                              informant_class=HiC_Loops_File, reference_informant_names=['Mustache'], description='observed_VC_SQRT_5000bp_diff_fdr2_0.05_mustache_fdr1_0.2_results_folder_February_21_2024',\n",
    "                                                                              genome_assembly_name='GRCh38', gz=False, species='homo_sapiens')\n",
    "\n",
    "informants_list += (loop_bedpes_list)\n",
    "\n",
    "this_informant_df.append(informants_list)\n",
    "\n",
    "this_informant_df.append([HiC_Loops_File(name='KO_VSMock.diffloop1.consensus.bedpe', description='Preliminary consensus loops produced between two Mustache loop calls at different normalizations and fdr rates and an arbitrary consensus score threshold at 10kbp resolution.', reference_informant_names=['Mustache'], tags=['consensus_features', 'EZH2_KO', 'hematopoietic_stem_cells'], genome_assembly_name='GRCh38', gz=False, species='homo_sapiens', location = '/home/cfrankston/Projects/consensus_features/consensus_features/results/KO_VS_Mock.diffloop1.consensus.bedpe')])\n",
    "\n",
    "print(this_informant_df.df.loc[6]['informant'].__dict__)\n",
    "this_informant_df.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n",
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n",
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n",
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n",
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n",
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n",
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n",
      "Error evaluating expression: isinstance expected 2 arguments, got 1\n"
     ]
    }
   ],
   "source": [
    "this_informant_df.df['verification_status'] = True\n",
    "this_informant_df.filter('isinstance(@informant, HiC_Loops_File)', additional_context={'HiC_Loops_File':HiC_Loops_File})\n",
    "\n",
    "this_informant_df.save_df(df_pkl_path='/home/cfrankston/Projects/hic_scope/informant_dataframes/hic_scope_test_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_trial_informant_df = informant_class.Informant_Dataframe()\n",
    "\n",
    "putative_loops_inf_df = informant_class.create_file_informant_list_from_folder(root_folder = \"/home/cfrankston/Projects/CEDAR_Projects/2024-02-21_EZH2-knockout-hic/data/data_March_13_2024\",\n",
    "use_location=True, attribute_sequence=['description','name'],informant_class=HiC_Loops_File, reference_informant_names=['Mustache'],\n",
    "                                                                              genome_assembly_name='GRCh38', gz=False, species='homo_sapiens')\n",
    "\n",
    "consensus_trial_informant_df.append(putative_loops_inf_df)\n",
    "consensus_trial_informant_df.df\n",
    "\n",
    "consensus_trial_informant_df.save_df(df_pkl_path='/home/cfrankston/Projects/consensus_features/consensus_features/informant_dataframes/loop_rep_infs_df')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'rep1_KO_VS_Mock.diffloop2',\n",
       " 'description': 'observed_KR_10000bp_diff_fdr2_0.05_mustache_fdr1_0.15_results_folder_March_7_2024',\n",
       " 'tags': [],\n",
       " 'reference_informant_names': ['Mustache'],\n",
       " 'informant_class': 'HiC_Loops_File',\n",
       " 'reference_informant_name_redundancy_values': {'Mustache': None},\n",
       " 'source_depth': 6,\n",
       " 'species': 'homo_sapiens',\n",
       " 'location': '/home/cfrankston/Projects/CEDAR_Projects/2024-02-21_EZH2-knockout-hic/data/observed_KR_10000bp_diff_fdr2_0.05_mustache_fdr1_0.15_results_folder_March_7_2024/KO_VS_Mock/rep1_KO_VS_Mock.diffloop2',\n",
       " 'external_locations': None,\n",
       " 'file_type': '.bedpe',\n",
       " 'genome_assembly_name': 'GRCh38',\n",
       " 'aliases': None,\n",
       " 'hic_file': None,\n",
       " 'feature_type': 'HiC_Loop',\n",
       " 'gz': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consensus_trial_informant_df.df['informant'][0].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Project using Imported Ontology and Desired Informant Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing command: \n",
      "python  launch_project_3.0.py test_launch_v3/version-1.0 /home/groups/CEDAR/franksto/ont_rdb/ont_rdb/informant_class.py /home/groups/CEDAR/franksto/ont_rdb/ont_rdb/ontologies/hic_January_24_2024_ontology.py /home/groups/CEDAR/franksto/ont_rdb/ont_rdb/explorer_auxiliaries.py ont_rdb/ont_rdb/ontology_dataframes/hic_January_24_2024_ontology_dataframe.pkl /home/groups/CEDAR/franksto\n",
      "Created project structure.\n",
      "Linked informant_class script, ontology and informants.\n",
      "Created metadata file.\n",
      "Copied consolidate_project.py.\n",
      "Creating explorer notebook for test_launch_v3/version-1.0 in /home/groups/CEDAR/franksto/test_launch_v3/version-1.0.\n",
      "Created explorer notebook.\n",
      "\n",
      "Project named test_launch_v3/version-1.0 is constructed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import snakemake\n",
    "\n",
    "project_name = \"test_launch_v3/version-1.0\"#\"PDAC_reprogramming_RNA/version-1.0\"\n",
    "#\"project_manager/version_1.0\"#\"transcription-from-3D-genome_attributions/version_1.0\"\n",
    "\n",
    "informant_class_path = \"/home/groups/CEDAR/franksto/ont_rdb/ont_rdb/informant_class.py\"\n",
    "\n",
    "ontology_script_path = '/home/groups/CEDAR/franksto/ont_rdb/ont_rdb/ontologies/hic_January_24_2024_ontology.py'#'/home/groups/CEDAR/franksto/ont_rdb/ont_rdb/ontologies/2024-11-11_RNA_seq_ontology.py'\n",
    "#\"/home/groups/CEDAR/franksto/ont_rdb/ont_rdb/ontologies/2024-8-15_hic_ontology.py\"\n",
    "#\"/home/groups/CEDAR/franksto/ont_rdb/ont_rdb/ontologies/project_manager_version_1_ontology.py\"\n",
    "\n",
    "explorer_auxiliaries_path = \"/home/groups/CEDAR/franksto/ont_rdb/ont_rdb/explorer_auxiliaries.py\"\n",
    "\n",
    "informant_dataframe_path = \"ont_rdb/ont_rdb/ontology_dataframes/hic_January_24_2024_ontology_dataframe.pkl\"\n",
    "base_directory = \"/home/groups/CEDAR/franksto\"#/2024-6-24\"\n",
    "\n",
    "meta_log = \"\"\n",
    "\n",
    "command = f\"python  launch_project_3.0.py {project_name} {informant_class_path} {ontology_script_path} {explorer_auxiliaries_path} {informant_dataframe_path} {base_directory}\"\n",
    "result = subprocess.run(command, capture_output=True, text=True, shell=True)\n",
    "\n",
    "print(f\"Processing command: \\n{command}\")\n",
    "# Check if the command was successful\n",
    "if result.returncode == 0:\n",
    "    # Print the standard output of the command\n",
    "    print(result.stdout)\n",
    "    print(f\"Project named {project_name} is constructed.\")\n",
    "else:\n",
    "    # Print the standard error if the command failed\n",
    "    print(\"Command failed with error:\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ont_rdb_env]",
   "language": "python",
   "name": "conda-env-ont_rdb_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
